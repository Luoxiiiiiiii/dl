{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "446c1662-5c99-45ed-a912-9913e0948497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5620,  0.5979,  1.4958,  0.1142, -1.6214], dtype=torch.float64,\n",
      "       requires_grad=True) tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([ 1.5620,  1.5979,  2.4958,  1.1142, -0.6214], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "1.2296897851380213\n",
      "\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.from_numpy(np.random.randn(5)).requires_grad_(True)\n",
    "y = torch.ones_like(x)\n",
    "print(x, y)\n",
    "\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "out = z.mean()\n",
    "print(f\"\\n\\n{out}\\n\")\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "38de8d13-2295-4d57-b437-0530e8433313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1484, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([1.2000, 1.2000, 1.2000, 1.2000, 1.2000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#backward()的累加效应\n",
    "out2 = x.sum()\n",
    "print(out2)\n",
    "out2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d9e25546-c7a9-41a0-b105-3fe8f9fef7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#清除x的梯度\n",
    "x.grad.data.zero_()\n",
    "out3 = x.sum()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "c858e79c-8e11-46e1-a2c5-799200c09985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6., 8.], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#x = torch.tensor([1, 2, 3, 4], requires_grad = True)  #会报错，整数不能求梯度\n",
    "x = torch.tensor([1., 2., 3., 4.], requires_grad = True)\n",
    "a = torch.tensor([2., 2., 2., 2.], requires_grad = True)\n",
    "y = a * x\n",
    "print(y)\n",
    "z = y.view(2, 2)\n",
    "print(z)\n",
    "#z.backward() #z不是标量，直接backward()会报错\n",
    "#需要传入一个与z同形状的v进行相乘求和成一个标量s才可以对s反向传播，公式如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedc3e1-8440-4df0-9a63-a158a89789aa",
   "metadata": {},
   "source": [
    "$$\n",
    "S = \\langle a, b \\rangle = \\sum z_{ij}v_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39b828-6f22-485a-bcd6-e0db0cdbf4e4",
   "metadata": {},
   "source": [
    "| 显示效果                         | LaTeX 代码                     |\r\n",
    "| ---------------------------- | ---------------------------- |\r\n",
    "| $(a+b)$                      | `(a+b)`                      |\r\n",
    "| $[a+b]$                      | `[a+b]`                      |\r\n",
    "| $\\{a+b\\}$                    | `\\{a+b\\}`                    |\r\n",
    "| $\\langle a, b \\rangle$       | `\\langle a, b \\rangle`       |\r\n",
    "| $\\vert x \\vert$              | `\\vert x \\vert`              |\r\n",
    "| $\\|x\\|$                      | `\\|x\\|`                      |\r\n",
    "| $\\lfloor x \\rfloor$          | `\\lfloor x \\rfloor`          |\r\n",
    "| $\\lceil x \\rceil$            | `\\lceil x \\rceil`            |\r\n",
    "| $\\langle z, v \\rangle$       | `\\langle z, v \\rangle`       |\r\n",
    "| $\\big( \\frac{a}{b} \\big)$    | `\\big( \\frac{a}{b} \\big)`    |\r\n",
    "| $\\Big[ \\frac{a}{b} \\Big]$    | `\\Big[ \\frac{a}{b} \\Big]`    |\r\n",
    "| $\\left( \\frac{a}{b} \\right)$ | `\\left( \\frac{a}{b} \\right)` |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "3ea6161e-5073-43c2-8969-df3b511e14d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward0>)\n",
      "tensor([  3.0000,   0.2000,   0.3933, 124.0000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对 z 或上游某个变量执行过 backward，图已经释放了，中间变量的值已经被释放了\n",
    "#要么保留中间变量，要么重新正向传播\n",
    "v = torch.tensor([[3, 0.1], [0.1311, 31]])\n",
    "#retain_graph = True可以保留计算图中变量的值\n",
    "z.backward(v, retain_graph = True)\n",
    "#z.backward(v, retain_graph = True)不会改变z本身的值\n",
    "print(z)\n",
    "print(a.grad)\n",
    "a.grad.data.zero_()\n",
    "\n",
    "#a.grad和a是同形的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002247c-b741-4f12-b234-345e47c54766",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{x}s= 2\\cdot\\mathrm{flatten}(v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ad9c64a9-491a-4930-8d0a-68e44d11ff1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith的简单理解\\n\\nwith expression [as variable]:\\n\\n    do_something()\\n\\nwith 语句特点是：\\n1.expression必须返回一个 上下文管理器（context manager）对象\\n2.as variable：可选，用来接收 __enter__() 方法返回的对象。\\n3.代码块执行完后会自动调用 __exit__() 方法做清理工作。\\n4.不论是否运行出错都会结束运行\\n5.化简\\n    try:\\n    finally:\\n   语法 \\n'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with的简单理解\n",
    "\n",
    "with expression [as variable]:\n",
    "\n",
    "    do_something()\n",
    "\n",
    "with 语句特点是：\n",
    "1.expression必须返回一个 上下文管理器（context manager）对象\n",
    "2.as variable：可选，用来接收 __enter__() 方法返回的对象。\n",
    "3.代码块执行完后会自动调用 __exit__() 方法做清理工作。\n",
    "4.不论是否运行出错都会结束运行\n",
    "5.化简\n",
    "    try:\n",
    "    finally:\n",
    "   语法 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "c2a1e082-7b26-4386-ae55-98c9b469e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('test.txt')\n",
    "try:\n",
    "    pass\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7b313d76-fa2c-4abe-b8df-30a3630f07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这段代码和前后没有关系请忽略，只是一个测试\n",
    "import os\n",
    "import platform\n",
    "\n",
    "file_path = \"test.txt\"\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    os.startfile(file_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "c83e030f-2a48-450a-a31a-e63283f77a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以化简为如下\n",
    "with open('test.txt') as f:  \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "0fa8f39a-8d70-4846-af26-f95129609d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering construtor of MyOpen\n",
      "Entering __enter__ of MyOpen\n",
      "Entering the __exit__ of MyOpen\n",
      "Caught a ValueError\n"
     ]
    }
   ],
   "source": [
    "#一个简单的例子说明with如何工作\n",
    "class MyOpen:#定义一个上下文管理器的类\n",
    "    def __init__(self, filepath):\n",
    "        print('Entering construtor of MyOpen') #验证进入了初始化\n",
    "        self.filepath = filepath\n",
    "    def __enter__(self):\n",
    "        print('Entering __enter__ of MyOpen') #验证进入了初始化\n",
    "        return self.filepath #就是把return的这个self.filepath给了f\n",
    "    def __exit__(self, exc_type, exc_value, traceback):#分别表示异常类型、异常实例、traceback，若没有异常，则都是None\n",
    "        print('Entering the __exit__ of MyOpen')\n",
    "        if exc_type is ValueError: #if语句后面记得接冒号  \n",
    "            print('Caught a ValueError')\n",
    "        return True\n",
    "\n",
    "with MyOpen('test.txt') as f:\n",
    "    raise ValueError('A ValueError occured') #加入异常仍能退出\n",
    "    print(f'The value of f is {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "a3210cab-bebe-47f8-a4c7-cc3b6c23a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward1>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "#x的n次方\n",
    "n = torch.tensor(2., requires_grad = True)\n",
    "m = torch.tensor(3., requires_grad = True)\n",
    "y1 = x ** n\n",
    "\n",
    "with torch.no_grad():\n",
    "    y2 = x ** m\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)#输出False，torch.no_grad()中断了requires_grad = True\n",
    "\n",
    "y3 = y1 + y2\n",
    "print(y3, y3.requires_grad)#True + False 还是True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "217da610-8e50-438d-aa6d-cc7e68a38ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(0.)\n",
      "True True False False\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(y2.grad)\\ny2.grad.data.zero_()\\nprint(m.grad)\\nm.grad.data.zero_()\\n'"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#保存梯度\n",
    "y1.retain_grad()\n",
    "y3.retain_grad()\n",
    "\n",
    "y3.backward(retain_graph = True)\n",
    "\n",
    "print(x.grad)\n",
    "x.grad.data.zero_()\n",
    "print(n.grad)\n",
    "n.grad.data.zero_()\n",
    "\n",
    "#对y1和y3直接输出.grad会报错，因为要节省内存会直接释放非子叶节点的梯度，可以选择手动保存\n",
    "print(x.is_leaf, n.is_leaf, y1.is_leaf, y3.is_leaf)\n",
    "print(y1.grad)\n",
    "y1.grad.data.zero_()\n",
    "print(y3.grad)\n",
    "y3.grad.data.zero_()\n",
    "\n",
    "#下面两个会报错，因为y2.requires_grad = False\n",
    "'''\n",
    "print(y2.grad)\n",
    "y2.grad.data.zero_()\n",
    "print(m.grad)\n",
    "m.grad.data.zero_()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a89dc9-cdbd-43ea-b819-c670e6b2381e",
   "metadata": {},
   "source": [
    "$$\n",
    "y_{3}=y_{1}+y_{2} = x^{5}\n",
    "$$\n",
    "\n",
    "但是x.grad却是$2x$,因为y_{2} = x^{3}的requires_grad是False，所以梯度不会回传，所以y2.backward()会报错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30740b01-1833-49b6-95ef-30a23dd4c925",
   "metadata": {},
   "source": [
    "### *注意*：\n",
    "1. 一次正向传播无法重复.backward(),因为会释放计算图节省空间，解决方法：\n",
    "<pre>    .backward(retain_graph=True)</pre>\n",
    "\n",
    "2. .backward()不会保留非叶子张量的梯度，因为节省空间，解决方法：\n",
    "<pre>    .retain_grad()\n",
    "    .backward()</pre>\n",
    "\n",
    "3. 梯度会累积，解决方法：\n",
    "<pre>    .backward()\n",
    "    .grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f75d7-9a3f-49fe-9d4c-08c41f66f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "要修改值建议用torch.no_grad()，不要用.data\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_env)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
